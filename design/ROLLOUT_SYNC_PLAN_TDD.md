# TDD: Index Rollout & Synchronization Plan

**Status:** Proposed
**Author:** AI Assistant (Gemini)
**Date:** 2025-05-01

## 1. Introduction & Motivation

The current MVP uses a manually generated index based on a one-time export. To make the RAG system useful long-term, we need a strategy to:
1.  Perform the initial indexing of the full dataset (~1600 entries).
2.  Keep the index reasonably up-to-date with new or modified entries in Notion.

This document explores options and proposes a plan for both initial rollout and ongoing synchronization. While scripts like `cli.py` and `build_index.py` will contain the core logic for Notion interaction and index creation, their direct use by end-users will be minimized. Instead, these will function as internal components, invoked by the application itself or automated backend processes, to simplify the end-user experience.

## 2. Goals

*   Define a process for indexing the entire Notion database.
*   Define a strategy for updating the index as Notion data changes.
*   Consider trade-offs between freshness, complexity, cost, and performance.

## 3. Non-Goals

*   Real-time, instantaneous synchronization.
*   Complex conflict resolution logic.
*   Full implementation details (this doc focuses on strategy).

## 4. Proposed Design / Technical Solution

### 4.1 Initial Rollout (Indexing 1600+ Entries) - **Chosen Strategy: Batch Processing**

After considering the risks associated with a single large export and index operation, the chosen strategy is a batch-based approach to enhance robustness and resumability. The core export and indexing logic, currently encapsulated in `cli.py` and `build_index.py`, will be utilized.

1.  **Batched Export (Utilizing Core Export Logic):**
    *   The core export logic (currently in `cli.py`) will be enhanced to support exporting data in smaller, time-based chunks (e.g., monthly).
    *   This logic will accept parameters like a target export period (e.g., `--month YYYY-MM` if invoked programmatically or via an orchestration script).
    *   Each invocation for a batch generates a separate JSON file (e.g., `output/2023-01.json`, `output/2023-02.json`, etc.).
    *   **Orchestration Process:** A separate script or automated process (e.g., part of a backend service or a deployment routine) will iterate through the desired time periods and invoke the batched export logic for each. This process manages the overall export.
    *   **Robustness:** Notion API retry logic will be maintained within each monthly export call. Failure in one batch does not halt the entire process.
    *   *Pros:* Mitigates API limits/timeouts, improves resumability, isolates errors to specific batches.
    *   *Cons:* Requires structuring the core export logic to be easily invokable and managing the orchestration process.

2.  **Incremental Index Building (Utilizing Core Indexing Logic):**
    *   The core indexing logic (currently in `build_index.py`) will be enhanced to process multiple input JSON files generated by the batched export.
    *   This logic will accept a list of files or a directory pattern (e.g., `output/*.json`).
    *   **Process:**
        *   Load existing `index.faiss` and `index_mapping.json` if they exist; otherwise, initialize new ones.
        *   Iterate through the input JSON files chronologically.
        *   For each file, process its entries (embedding with OpenAI batching/retries).
            *   **Identify Max `last_edited_time`:** During this processing, keep track of the maximum `last_edited_time` encountered among the successfully processed entries in the current batch.
        *   Add new vectors and mappings to the loaded index/mapping.
        *   **Crucially, save the updated index and mapping files back to disk *after successfully processing each input JSON file*.** This acts as a checkpoint.
        *   The overall maximum `last_edited_time` from all processed batches should be recorded as the "Last Processed Entry Timestamp" (see Section 4.2.1.b).
    *   *Pros:* Handles large datasets without excessive memory usage, highly resumable, leverages existing embedding logic.
    *   *Cons:* Requires modifications to the core indexing logic for incremental loading, saving, and `last_edited_time` tracking.

### 4.2 Ongoing Synchronization - **Chosen Strategy: Incremental Updates via Polling**

The chosen strategy for ongoing synchronization is an incremental update approach, triggered either by application startup (for local use) or a scheduled job (for hosted services). This method aims to balance data freshness with efficiency by only processing changes since the last successful sync. This approach is based on the principles of "Option C: Incremental Updates (Batch Polling)" and incorporates ideas from external suggestions for an "application startup sync."

**Core Mechanism:**

1.  **Timestamp Tracking:**
    *   a. **Last Successful Sync Timestamp:**
        *   A record of the last successful synchronization operation's timestamp is maintained.
        *   **Local:** Stored in a file (e.g., `last_sync_timestamp.txt`).
        *   **Hosted (e.g., Vercel):** Stored in a persistent key-value store (e.g., Vercel KV, Upstash Redis).
    *   b. **Last Processed Entry Timestamp (for Frontend Display):**
        *   A record of the `last_edited_time` of the most recent Notion entry successfully processed and indexed across all sync operations. This is used to display "Last Updated" information in the frontend.
        *   **Storage:** Maintained similarly to the `last_sync_timestamp` (e.g., `last_entry_update_timestamp.txt` or a separate KV store entry).
        *   This timestamp is updated at the end of any successful sync operation (initial or incremental) that processes new or modified entries.

2.  **Sync Trigger & Changed Data Export:**
    *   **Local:** When the application starts, it checks the `last_sync_timestamp`. If it exceeds a defined threshold (e.g., 24 hours), a sync is triggered by the application invoking the necessary backend logic.
    *   **Hosted:** A scheduled job (e.g., Vercel Cron Job, GitHub Action) runs periodically (e.g., daily), invoking the backend export logic.
    *   The core export logic (from `cli.py`) is invoked to export only entries modified since the last recorded `last_sync_timestamp`. This will use Notion's `last_edited_time` property for filtering.
        *   The export logic needs to accept a parameter like `--since-timestamp <ISO_DATETIME_STRING>` or a flag indicating to use the stored `last_sync_timestamp`.
    *   Exported data is saved to a temporary JSON file (e.g., `incremental_sync_updates.json`).

3.  **Incremental Index Update (Utilizing Core Indexing Logic):**
    *   The core indexing logic (from `build_index.py`) is modified/invoked to process the `incremental_sync_updates.json` and update the existing FAISS index and `index_mapping.json`.
    *   **Process:**
        *   Load the existing `index.faiss` and `index_mapping.json`.
        *   Initialize a variable to track the maximum `last_edited_time` for the current incremental batch (e.g., `current_batch_max_last_edited_time`).
        *   For each entry in the incremental JSON:
            *   **Check for existing entry:** Use the Notion Page ID to see if the entry already exists in `index_mapping.json`.
            *   **If existing (Updated Page):**
                *   Retrieve the current FAISS vector ID(s) for this Notion Page ID from `index_mapping.json`.
                *   Remove these old vector(s) from the FAISS index using `FAISS_INDEX.remove_ids(...)`. (This requires the FAISS index to have been built using `add_with_ids`).
                *   Update/remove the entry from `index_mapping.json`.
            *   **Embed content:** Generate new embeddings for the new or updated page content.
            *   **Add to FAISS:** Add the new vector(s) to the FAISS index using `FAISS_INDEX.add_with_ids(...)`, storing the new unique FAISS IDs.
            *   **Update mapping:** Add/update the Notion Page ID to new FAISS vector ID(s) mapping in `index_mapping.json`.
            *   **Track Max `last_edited_time`:** Update `current_batch_max_last_edited_time` if the current entry's `last_edited_time` is newer.
        *   Save the updated `index.faiss` and `index_mapping.json` back to disk (or blob storage for hosted environments).

4.  **Sync Completion:**
    *   After the sync operation completes successfully:
        *   Update the `last_sync_timestamp` in its respective storage location.
        *   Update the `last_entry_update_timestamp` with the `current_batch_max_last_edited_time` (or the overall max from initial rollout) if it's newer than the currently stored value.
    *   **User Notification (Local/Optional for Hosted):** During a startup sync (local) or if a user queries data while a background sync recently ran, the application can inform the user. The frontend can also independently display the `last_entry_update_timestamp`.

**Pros:**
*   **Efficiency:** Only processes and embeds changed data, reducing API calls and computational load compared to full re-indexing.
*   **Fresher Data:** Keeps the index more up-to-date than infrequent full re-builds.
*   **Balanced Complexity:** More manageable than real-time event-driven systems, while providing good freshness.
*   **Adaptable:** Works for local CLI and can be adapted for scheduled jobs in a hosted environment.

**Cons/Considerations:**
*   **State Management:** Requires careful management of the `last_sync_timestamp` and `last_entry_update_timestamp`.
*   **Index Update Logic:** The core indexing logic (`build_index.py`) needs robust logic for adding, and critically, updating/removing existing entries in FAISS, which depends on a reliable mapping between Notion page IDs and FAISS vector IDs. It also needs to correctly identify and propagate the `last_edited_time` of processed entries.
*   **Handling Deletions:** This strategy primarily addresses new and modified pages. A separate strategy for handling deleted Notion pages will be needed (e.g., periodic full comparison, or attempting to infer deletions if pages disappear from API responses â€“ though the latter is less reliable).
*   **Initial FAISS Setup:** The FAISS index should be initialized to support `add_with_ids` and `remove_ids`.
*   **Mechanism/API for the frontend to securely and efficiently retrieve the `last_entry_update_timestamp` for display.**
*   **Ensuring the `last_entry_update_timestamp` accurately reflects the latest data visible to the user through the index.**

### 4.3 Alternatives Considered for Ongoing Sync

*   **Option A: Periodic Full Re-indexing:**
    *   Schedule a job (e.g., cron, GitHub Action) to periodically (daily? weekly?) run the full export and `build_index.py` process.
    *   *Pros:* Simplest to implement.
    *   *Cons:* Index can be stale; inefficient (re-embeds unchanged entries); potentially costly if run frequently.
*   **Option B: Incremental Updates (Event-Driven - Harder):**
    *   Requires a way to detect changes in Notion (Webhooks? More frequent, targeted polling?). Notion Webhooks, if available and suitable, could be an option but add complexity.
    *   Polling: Periodically query Notion for pages modified since the last check (`last_edited_time` filter).
    *   For modified pages: Re-fetch content, re-embed, update/replace vector in FAISS.
    *   For new pages: Fetch, embed, add.
    *   *Pros:* Index is potentially the freshest; most efficient in terms of data processing if events are granular.
    *   *Cons:* Much more complex to implement reliably; requires robust state management; handling deletions can be tricky; Notion webhook support/reliability for this specific use case needs verification.

## 5. Alternatives Considered (General)

*   Using a managed Vector DB service (Pinecone, etc.) - Might simplify sync later, but adds external dependency.

## 6. Impact / Risks / Open Questions

*   Cost of embedding 1600+ entries initially?
*   Cost of frequent re-embedding for sync?
*   Performance of FAISS with 1600+ entries?
*   Reliability of Notion API for large queries/frequent polling?
*   Complexity of implementing incremental updates vs. stale data tolerance?
*   How to handle Notion page deletions?
*   Mechanism/API for the frontend to securely and efficiently retrieve the `last_entry_update_timestamp` for display.
*   Ensuring the `last_entry_update_timestamp` accurately reflects the latest data visible to the user through the index.

## 7. Dependencies to Add (Potentially)

*   Scheduling library (e.g., `schedule`, `apscheduler`) if polling/periodic sync is chosen.
*   Libraries for specific Vector DBs if not using FAISS. 