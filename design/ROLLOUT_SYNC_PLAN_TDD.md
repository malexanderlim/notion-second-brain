# TDD: Index Rollout & Synchronization Plan

**Status:** Proposed
**Author:** AI Assistant (Gemini)
**Date:** 2025-05-01

## 1. Introduction & Motivation

The current MVP uses a manually generated index based on a one-time export. To make the RAG system useful long-term, we need a strategy to:
1.  Perform the initial indexing of the full dataset (~1600 entries).
2.  Keep the index reasonably up-to-date with new or modified entries in Notion.

This document explores options and proposes a plan for both initial rollout and ongoing synchronization.

## 2. Goals

*   Define a process for indexing the entire Notion database.
*   Define a strategy for updating the index as Notion data changes.
*   Consider trade-offs between freshness, complexity, cost, and performance.

## 3. Non-Goals

*   Real-time, instantaneous synchronization.
*   Complex conflict resolution logic.
*   Full implementation details (this doc focuses on strategy).

## 4. Proposed Design / Technical Solution

### 4.1 Initial Rollout (Indexing 1600+ Entries) - **Chosen Strategy: Batch Processing**

After considering the risks associated with a single large export and index operation, the chosen strategy is a batch-based approach to enhance robustness and resumability:

1.  **Batched Export (`cli.py` Enhancement):**
    *   Modify `cli.py` to support exporting data in smaller, time-based chunks (e.g., monthly).
    *   Introduce a new command-line argument, such as `--month YYYY-MM`, to specify the target export period.
    *   Each execution generates a separate JSON file (e.g., `output/2023-01.json`, `output/2023-02.json`, etc.).
    *   **Control Script:** A separate script (shell or Python) will be needed to iterate through the desired time periods (e.g., all months from the start date to the present) and invoke `cli.py --export --month ...` for each. This script manages the overall export process.
    *   **Robustness:** Maintain Notion API retry logic within each monthly export call. Failure in one batch does not halt the entire process.
    *   *Pros:* Mitigates API limits/timeouts, improves resumability, isolates errors to specific batches.
    *   *Cons:* Requires modifying `cli.py` and creating a control script.

2.  **Incremental Index Building (`build_index.py` Enhancement):**
    *   Modify `build_index.py` to process multiple input JSON files generated by the batched export.
    *   The script will accept a list of files or a directory pattern (e.g., `output/*.json`).
    *   **Process:**
        *   Load existing `index.faiss` and `index_mapping.json` if they exist; otherwise, initialize new ones.
        *   Iterate through the input JSON files chronologically.
        *   For each file, process its entries (embedding with OpenAI batching/retries).
        *   Add new vectors and mappings to the loaded index/mapping.
        *   **Crucially, save the updated index and mapping files back to disk *after successfully processing each input JSON file*.** This acts as a checkpoint.
    *   *Pros:* Handles large datasets without excessive memory usage, highly resumable, leverages existing embedding logic.
    *   *Cons:* Requires modifications to `build_index.py` for incremental loading and saving.

### 4.2 Ongoing Synchronization

*   **Option A: Periodic Re-indexing:**
    *   Schedule a job (e.g., cron, GitHub Action) to periodically (daily? weekly?) run the full export and `build_index.py` process.
    *   *Pros:* Simplest to implement.
    *   *Cons:* Index can be stale; inefficient (re-embeds unchanged entries); potentially costly if run frequently.
*   **Option B: Incremental Updates (Event-Driven - Harder):**
    *   Requires a way to detect changes in Notion (Webhooks? Periodic polling?). Notion Webhooks (if available/suitable) might be complex.
    *   Polling: Periodically query Notion for pages modified since the last check (`last_edited_time` filter).
    *   For modified pages: Re-fetch content, re-embed, update/replace vector in FAISS (requires unique ID mapping, FAISS `remove_ids` / `add_with_ids`).
    *   For new pages: Fetch, embed, add.
    *   For deleted pages: Need strategy (e.g., periodic cleanup or check during retrieval).
    *   *Pros:* Index is fresher; more efficient potentially.
    *   *Cons:* Much more complex to implement reliably; requires state management (last sync time); handling deletions is tricky.
*   **Option C: Incremental Updates (Batch Polling):**
    *   Periodically query Notion for pages modified since last check.
    *   Export *only* these modified/new pages to a temporary JSON.
    *   Run a modified `build_index.py` that can *update* an existing FAISS index (remove old vectors, add new ones).
    *   *Pros:* Balances freshness and complexity; less complex than event-driven.
    *   *Cons:* Still requires state management and index update logic.

## 5. Alternatives Considered

*   Using a managed Vector DB service (Pinecone, etc.) - Might simplify sync later, but adds external dependency.

## 6. Impact / Risks / Open Questions

*   Cost of embedding 1600+ entries initially?
*   Cost of frequent re-embedding for sync?
*   Performance of FAISS with 1600+ entries?
*   Reliability of Notion API for large queries/frequent polling?
*   Complexity of implementing incremental updates vs. stale data tolerance?
*   How to handle Notion page deletions?

## 7. Dependencies to Add (Potentially)

*   Scheduling library (e.g., `schedule`, `apscheduler`) if polling/periodic sync is chosen.
*   Libraries for specific Vector DBs if not using FAISS. 