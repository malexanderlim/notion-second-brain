# TDD: MVP RAG Implementation for Notion Second Brain

**Status:** Proposed
**Author:** AI Assistant (Gemini) & User
**Date:** 2025-05-01 (Approx Midnight!)

## 1. Introduction & Motivation

The goal is to enable a Minimum Viable Product (MVP) demonstration of querying the Notion Journal ("Second Brain") using natural language by tomorrow morning. This requires the quickest possible implementation of a Retrieval-Augmented Generation (RAG) pipeline integrated into the existing CLI tool. This document outlines the technical design for this rapid MVP.

## 2. Goals

*   Implement a basic RAG pipeline capable of answering natural language questions based on journal entries.
*   Integrate this functionality into `cli.py` via a `--query` argument.
*   Use readily available, easy-to-set-up components suitable for a fast turnaround.
*   Produce a demonstrable result even if the implementation is not robust or optimized.
*   Focus solely on retrieving information from the content of existing exported JSON data.

## 3. Non-Goals (for MVP)

*   Robust testing (unit, integration).
*   Scalable vector database solution.
*   Incremental indexing or real-time sync with Notion.
*   Advanced document chunking strategies.
*   Metadata filtering during retrieval.
*   Sophisticated prompt engineering or context window management.
*   Comprehensive error handling beyond basic API call resilience.
*   Extensive logging for the RAG components.
*   Web interface or alternative query methods.
*   Handling images, complex block types beyond basic text, or other Phase 2+ features.
*   Optimizing performance or cost.

## 4. Proposed Design

The MVP RAG pipeline consists of two main parts: offline index creation and online query handling.

### 4.1 Offline Index Creation (`build_index.py`)

A new script, `build_index.py`, will be created to perform a one-time setup for the demo.

1.  **Dependencies:**
    *   `openai`: For embedding generation.
    *   `faiss-cpu`: For local vector indexing.
    *   `python-dotenv`: To load API keys.
2.  **Input:** The existing JSON export file (e.g., `output/all_time.json` generated by `cli.py`).
3.  **Process:**
    *   Load the `entries` list from the JSON file.
    *   Initialize OpenAI client (requires `OPENAI_API_KEY` in `.env`).
    *   **Embedding:** For each entry, extract the `content` field. Send this text to the OpenAI Embeddings API (model: `text-embedding-ada-002`) to get a vector representation. Handle potential API errors with basic retries if needed.
    *   **Data Mapping:** Store the generated vectors and create a mapping structure. This mapping will link the index position of a vector back to its corresponding simple entry data (at minimum `page_id`, `title`, `entry_date`, `content`). A simple list of dictionaries matching the vector order could work.
    *   **Indexing:** Initialize a FAISS index (e.g., `IndexFlatL2`). Add the generated vectors to the index.
    *   **Output:**
        *   Save the populated FAISS index to a file (e.g., `index.faiss`).
        *   Save the data mapping structure to a file (e.g., `index_mapping.json`).
4.  **Execution:** This script will be run manually once before the demo to prepare the index files.

### 4.2 Online Query Handling (`cli.py`)

The existing `cli.py` script will be modified.

1.  **Dependencies:**
    *   `openai`: For embedding the query and generating the answer.
    *   `faiss-cpu`: To load the index and perform similarity search.
2.  **Argument Parsing:**
    *   Add a new argument: `--query "<natural language question>"`.
    *   Modify `main()` to check if `--query` is provided. If so, execute the RAG pipeline instead of the export logic.
3.  **Process (if `--query` is used):**
    *   Load the pre-built FAISS index from `index.faiss`.
    *   Load the data mapping from `index_mapping.json`.
    *   Initialize OpenAI client.
    *   **Query Embedding:** Embed the user's `--query` text using the same OpenAI embedding model (`text-embedding-ada-002`).
    *   **Retrieval:** Perform a similarity search (e.g., `index.search`) using the query vector against the loaded FAISS index. Retrieve the indices of the top `k` (e.g., k=3 to 5) most similar entries.
    *   **Context Fetching:** Use the retrieved indices to look up the corresponding entry data (specifically `title`, `entry_date`, `content`) from the loaded `index_mapping.json`. Format this retrieved data into a single context string.
    *   **Prompting:** Construct a simple prompt for the LLM. Example:
        ```
        You are a helpful assistant answering questions based on provided journal entries.

        Use the following journal entries to answer the question below:
        --- START CONTEXT ---
        {formatted_retrieved_entries_text}
        --- END CONTEXT ---

        Question: {user_query}
        Answer:
        ```
    *   **Generation:** Send this prompt to the OpenAI Chat Completion API (e.g., model `gpt-3.5-turbo` or `gpt-4o`).
    *   **Output:** Print the text content of the LLM's response directly to the console.
4.  **Error Handling:** Basic `try...except` blocks around API calls and file loading.

## 5. Alternatives Considered

*   **Different Vector DBs:** Pinecone, Weaviate, ChromaDB. Rejected for MVP due to slightly higher setup overhead compared to local FAISS.
*   **Different Embedding Models:** Sentence Transformers (local). Rejected for MVP to avoid local model download/setup complexity; OpenAI API is faster to integrate.
*   **Different LLMs:** Local models (Llama, etc.). Rejected for MVP due to setup complexity.
*   **Real-time Indexing:** Directly querying/embedding Notion content on the fly. Rejected due to complexity and potential rate-limiting issues during a demo. Batch indexing from JSON is simpler for MVP.

## 6. Future Work / Open Questions (Post-MVP)

*   **Testing:** Implement comprehensive unit and integration tests.
*   **Scalability:** Evaluate and potentially migrate to a more robust vector database.
*   **Syncing:** Design and implement incremental updates to the index.
*   **Chunking:** Explore better strategies than embedding entire entry content.
*   **Metadata:** Incorporate metadata filtering (dates, tags) into retrieval.
*   **Prompting:** Develop more sophisticated prompt templates and context management.
*   **Error Handling:** Make error handling more granular and user-friendly.
*   **UI:** Consider a web interface or other query methods.
*   **Cost/Performance:** Analyze and optimize API usage and indexing/query speed.
*   **Configuration:** Make index paths, models, top-k values configurable.
*   **(Note:** The actual implementation evolved significantly beyond this initial MVP, incorporating metadata pre-filtering and dynamic value analysis. See `TECHNICAL_LEARNINGS.md` for details on the final hybrid approach.)

## 7. Dependencies to Add

*   `